{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ae095-badb-4273-82fa-27d68785a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import product\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc35775-0d49-4b45-8f77-a202bcf8b305",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da62d64-2696-4dae-b23b-4efb81213a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size categories\n",
    "def classify_radius(size):\n",
    "    if size <= 15:\n",
    "        return 0.1  # Small size\n",
    "    elif 16 <= size <= 140:\n",
    "        return 0.3 # Medium size\n",
    "    else:\n",
    "        return 0.5  # Large size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6955e5b0-b687-4907-80c9-a7c1d475333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jitter to coordinates to reduce overlap\n",
    "def jitter_coordinates(lat, lon, max_jitter=3.5):\n",
    "    jittered_lat = lat + np.random.uniform(-max_jitter, max_jitter)\n",
    "    jittered_lon = lon + np.random.uniform(-max_jitter, max_jitter)\n",
    "    return jittered_lat, jittered_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd7cb2-7057-4c37-a2fc-ebb658db0836",
   "metadata": {},
   "source": [
    "# Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81711fe0-9fa7-4432-a169-b5a659fe0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_cluster = 'default'\n",
    "name = ''\n",
    "save = False\n",
    "save_data = False\n",
    "\n",
    "if optimal_cluster == 'default':\n",
    "    optimal_cluster = 'st_cluster_3_5_7'\n",
    "    name = ''\n",
    "    folder = ''\n",
    "else:\n",
    "    folder = 'SI/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8778350-c34a-4669-89cc-c92dd0c1e6ce",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ca8f56-9b75-42fe-8d67-fb0a992d9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2811/935040874.py:1: DtypeWarning: Columns (8,26,33,36,39,43,52,53,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  claims_clusters = pd.read_csv('clustered_claims_final.csv')\n"
     ]
    }
   ],
   "source": [
    "claims_clusters = pd.read_csv('clustered_claims_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deddb126-be19-4307-a336-0e4d5abd75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Coerce dtypes\n",
    "num_cols = [\"latitude\", \"longitude\", \"adjustedClaim\"]\n",
    "claims_clusters[num_cols] = claims_clusters[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "claims_clusters[\"dateOfLoss\"] = pd.to_datetime(claims_clusters[\"dateOfLoss\"], errors=\"coerce\")\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].astype(int).astype(str)\n",
    "claims_clusters['countyCode'] = claims_clusters['countyCode'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796f2dbd-5440-4914-bd69-62baca96933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_hist = 831697554.47 # From NFIP Historic Policies\n",
    "sum_2024 = 3879323216 # From NFIP RR2 Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db3d74-557a-4577-afe4-a3d650cc73ab",
   "metadata": {},
   "source": [
    "# Geospatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffad9626-dfaf-4405-ad85-fdc55e818a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the county shapefile\n",
    "county_shapefile_path = '../Local_Data/Geospatial/tl_2019_us_county.shp'\n",
    "gdf_counties = gpd.read_file(county_shapefile_path)\n",
    "\n",
    "# Load the shapefile for US states\n",
    "state_shapefile_path = '../Local_Data/Geospatial/cb_2018_us_state_20m.shp'\n",
    "gdf_states = gpd.read_file(state_shapefile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002ff0a-9be0-4d65-810e-4eb492c639ae",
   "metadata": {},
   "source": [
    "# Using Storm Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3998ee-67f2-485f-afac-bcde8a9a9684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile of cluster sizes: 141.80000000000018\n",
      "50th Percentile (Median) of cluster sizes: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Group by optimal_cluster and calculate medians and sizes\n",
    "cluster_centers = claims_clusters.groupby(optimal_cluster).agg(\n",
    "    median_latitude=('latitude', 'median'),\n",
    "    median_longitude=('longitude', 'median'),\n",
    "    cluster_size=('latitude', 'size'),  # Count rows in each cluster\n",
    "    claim_sum=('adjustedClaim', 'sum'), # Median of the claim_sum for color mapping\n",
    "    median_date=(\"dateOfLoss\", \"median\") \n",
    ").reset_index()\n",
    "\n",
    "# Calculate the 90th and 50th percentiles of the cluster_size\n",
    "percentile_90 = np.percentile(cluster_centers['cluster_size'], 90)\n",
    "percentile_50 = np.percentile(cluster_centers['cluster_size'], 50)\n",
    "\n",
    "# Print the results\n",
    "print(f\"90th Percentile of cluster sizes: {percentile_90}\")\n",
    "print(f\"50th Percentile (Median) of cluster sizes: {percentile_50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf4645-699c-4a00-90a8-5ca02e072c92",
   "metadata": {},
   "source": [
    "## Calculations of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e929d45-fb23-41bf-82d0-ca41c130994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2811/2707086539.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_8_clusters['Event'] = [\"Hurricane Katrina\", \"Hurricane Sandy\", \"Hurricane Harvey\", \"Hurricane Ian\", \"Hurricane Ike\", \"2016 Louisiana Flooding\", \"Hurricanes Ivan/Jeanne\", \"Hurricane Helene\"]\n"
     ]
    }
   ],
   "source": [
    "# Sort by claim_sum value to plot higher values last\n",
    "cluster_centers = cluster_centers.sort_values(by='claim_sum', ascending=True)\n",
    "\n",
    "# Classify cluster sizes into the three categories\n",
    "cluster_centers['radius'] = cluster_centers['cluster_size'].apply(classify_radius)\n",
    "\n",
    "sum_unclustered = claims_clusters[claims_clusters[optimal_cluster] == -1]['adjustedClaim'].sum()\n",
    "annual_unclustered = float(sum_unclustered)/(claims_clusters['yearOfLoss'].max()-claims_clusters['yearOfLoss'].min())\n",
    "\n",
    "average_claim_sum = cluster_centers['claim_sum'].mean()\n",
    "\n",
    "# Total sum of 'claim_sum'\n",
    "total_claim_sum = cluster_centers['claim_sum'].sum()\n",
    "top_10_sum = cluster_centers.nlargest(10, 'claim_sum')['claim_sum'].sum()\n",
    "top_8_sum = cluster_centers.nlargest(8, 'claim_sum')['claim_sum'].sum()\n",
    "\n",
    "# Sum of all others except the top 20\n",
    "sum_except_top_10 = total_claim_sum - top_10_sum\n",
    "sum_except_top_8 = total_claim_sum - top_8_sum\n",
    "\n",
    "# Extract year from the 'median_date' column\n",
    "cluster_centers['year'] = pd.to_datetime(cluster_centers['median_date']).dt.year\n",
    "\n",
    "# Group by year and count clusters\n",
    "clusters_per_year = cluster_centers.groupby('year')[optimal_cluster].nunique()\n",
    "\n",
    "# Calculate the average number of clusters per year\n",
    "average_clusters_per_year = clusters_per_year.mean()\n",
    "\n",
    "# Sort the DataFrame by 'claim_sum' in descending order\n",
    "cluster_centers = cluster_centers.sort_values(by='claim_sum', ascending=False)\n",
    "\n",
    "# Select the top 8 rows (99.9%)\n",
    "top_8_clusters = cluster_centers.head(8)\n",
    "\n",
    "# Assign names to the rows\n",
    "top_8_clusters['Event'] = [\"Hurricane Katrina\", \"Hurricane Sandy\", \"Hurricane Harvey\", \"Hurricane Ian\", \"Hurricane Ike\", \"2016 Louisiana Flooding\", \"Hurricanes Ivan/Jeanne\", \"Hurricane Helene\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdbe4cde-02de-472f-b5b3-167732f4225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the first chart\n",
    "df1 = pd.DataFrame({\n",
    "    \"Type\": [\"Unclustered\", \"Mean\\nCluster\"],\n",
    "    \"Value\": [annual_unclustered/1e6, average_claim_sum/1e6]\n",
    "})\n",
    "\n",
    "# Data for the second chart\n",
    "df2 = pd.DataFrame({\n",
    "    \"Type\": [\"99.9% Hyperclusters\", \"All Other Claims\"],\n",
    "    \"Value\": [top_8_sum/1e9, sum_except_top_8/1e9]\n",
    "})\n",
    "\n",
    "# --- Top 8 ---\n",
    "top_8_clusters = top_8_clusters.copy().reset_index(drop=True)\n",
    "if len(top_8_clusters) != 8:\n",
    "    raise ValueError(f\"Expected 8 rows in top_8_clusters, found {len(top_8_clusters)}\")\n",
    "\n",
    "top_8_clusters[\"Event\"] = [\n",
    "    \"Hurricane\\nKatrina\", \"Hurricane\\nSandy\", \"Hurricane\\nHarvey\", \"Hurricane\\nIan\",\n",
    "    \"Hurricane\\nIke\", \"Louisiana\\nFlood 2016\", \"Hurricanes\\nIvan+Jeanne\", \"Hurricane\\nHelene\"\n",
    "]\n",
    "top_8_clusters[\"claim_sum\"] = pd.to_numeric(top_8_clusters[\"claim_sum\"], errors=\"coerce\") / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d84f0a-857b-4c79-9063-7622039d6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'claim_sum' in descending order\n",
    "cluster_centers = cluster_centers.sort_values(by='claim_sum', ascending=False)\n",
    "\n",
    "# Select the top 12 rows\n",
    "billion_dollar = cluster_centers[cluster_centers[optimal_cluster] != -1].head(12)\n",
    "\n",
    "# Sort the DataFrame by 'claim_sum' in ascending order\n",
    "billion_dollar = billion_dollar.sort_values(by='claim_sum', ascending=True)\n",
    "\n",
    "# Extract year from 'median_date' and add it as a new column\n",
    "billion_dollar['year'] = pd.to_datetime(billion_dollar['median_date']).dt.year\n",
    "\n",
    "# Define radius using log scale for 'claim_sum'\n",
    "min_claim_sum = billion_dollar['claim_sum'].min()\n",
    "log_base = 10  # Adjust the base for scaling if needed\n",
    "max_radius = 3  # Maximum radius for visualization\n",
    "billion_dollar['radius'] = (\n",
    "    billion_dollar['cluster_size'] / billion_dollar['cluster_size'].max() * max_radius\n",
    ")\n",
    "\n",
    "# Apply jittering to each cluster's coordinates\n",
    "billion_dollar[['jittered_latitude', 'jittered_longitude']] = billion_dollar.apply(\n",
    "    lambda row: jitter_coordinates(row['median_latitude'], row['median_longitude']), axis=1, result_type=\"expand\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf731e1-394a-4e0e-bf21-3c0087727ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2811/468505537.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cluster_centers['claim_sum'].fillna(1e4, inplace=True)  # Replace NaN with a small value (e.g., 1e-1)\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/geopandas/plotting.py:480: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n",
      "  ax.figure.canvas.draw_idle()\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/geopandas/plotting.py:480: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n",
      "  ax.figure.canvas.draw_idle()\n"
     ]
    }
   ],
   "source": [
    "# Define the overall figure with a grid layout\n",
    "fig = plt.figure(figsize=(12, 8), constrained_layout=True)\n",
    "gs = GridSpec(2, 2, width_ratios=[2, 1])  # 2 rows, 2 columns with narrow barplot column\n",
    "\n",
    "# Limit to contiguous US (approximate bounding box)\n",
    "extent = [-125, -66.5, 24, 49]  # [min_lon, max_lon, min_lat, max_lat]\n",
    "\n",
    "# First column: Maps\n",
    "ax_map1 = fig.add_subplot(gs[0, 0])\n",
    "ax_map2 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Second column: Nested GridSpec for bar plots\n",
    "gs_bar = GridSpecFromSubplotSpec(2, 1, subplot_spec=gs[:, 1], height_ratios=[1, 1], hspace=0.3)  # Adjusted hspace\n",
    "bar_axes = [fig.add_subplot(gs_bar[i, 0]) for i in range(2)]  # Create individual bar plot axes\n",
    "\n",
    "# Sort by claim_sum value to plot higher values last\n",
    "cluster_centers = cluster_centers.sort_values(by='claim_sum', ascending=True)\n",
    "\n",
    "# Determine the min and max of the claim_sum\n",
    "cluster_centers['claim_sum'] = cluster_centers['claim_sum'].replace([np.inf, -np.inf], np.nan)\n",
    "cluster_centers['claim_sum'].fillna(1e4, inplace=True)  # Replace NaN with a small value (e.g., 1e-1)\n",
    "cluster_centers['claim_sum'] = cluster_centers['claim_sum'].apply(lambda x: max(x, 1e4))  # Replace with a small positive number\n",
    "vmin = cluster_centers['claim_sum'].min()\n",
    "vmax = cluster_centers['claim_sum'].max()\n",
    "\n",
    "# Update color map to yellow-to-red\n",
    "cmap_yellow_red = plt.cm.YlOrRd\n",
    "\n",
    "# Log normalization for color mapping\n",
    "norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Map the claim_sum to colors\n",
    "cluster_centers['color'] = cluster_centers['claim_sum'].apply(lambda x: cmap_yellow_red(norm(x)) if not np.isnan(x) else 'gray')\n",
    "\n",
    "# Plot 1: Map of all clusters\n",
    "gdf_counties.plot(ax=ax_map1, color='lightgray', edgecolor='gray', alpha=0.5)\n",
    "gdf_states.boundary.plot(ax=ax_map1, color='black', linewidth=0.5)\n",
    "\n",
    "patches1 = []\n",
    "for _, row in cluster_centers.iterrows():\n",
    "    center = (row['median_longitude'], row['median_latitude'])\n",
    "    radius = row['radius']\n",
    "    color = row['color']\n",
    "    circle = Circle(center, radius, edgecolor=color, facecolor=color, alpha=0.6)\n",
    "    patches1.append(circle)\n",
    "\n",
    "p1 = PatchCollection(patches1, match_original=True, transform=ax_map1.transData)\n",
    "ax_map1.add_collection(p1)\n",
    "\n",
    "ax_map1.set_xlim(extent[0], extent[1])\n",
    "ax_map1.set_ylim(extent[2], extent[3])\n",
    "ax_map1.axis('off')\n",
    "ax_map1.set_title(\"Clustered Claims\", fontsize=16)\n",
    "\n",
    "# Add subplot label \"a)\" in the upper left\n",
    "ax_map1.text(-0.1, 1, \"a)\", transform=ax_map1.transAxes, fontsize=14, va='top', ha='left')\n",
    "\n",
    "# Add legend for point sizes with varying sizes\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=4, label='Size ≤ 15', linestyle='None', alpha=0.5),\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=8, label='15 < Size < 140', linestyle='None', alpha=0.5),\n",
    "    Line2D([0], [0], marker='o', color='gray', markersize=12, label='Size ≥ 140', linestyle='None', alpha=0.5),\n",
    "]\n",
    "\n",
    "legend = ax_map1.legend(handles=legend_elements, loc='lower left', title='Claims per Cluster', frameon=True, fontsize=8)\n",
    "ax_map1.add_artist(legend)\n",
    "\n",
    "# Add first colorbar (left-aligned for the first map)\n",
    "cbar_ax1 = fig.add_axes([0.05, 0.55, 0.02, 0.3])  # [left, bottom, width, height]\n",
    "sm1 = plt.cm.ScalarMappable(cmap=cmap_yellow_red, norm=norm)\n",
    "cb1 = fig.colorbar(sm1, cax=cbar_ax1, orientation='vertical')\n",
    "cb1.ax.tick_params(labelsize=8)\n",
    "cb1.set_label('Cost ($)', fontsize=10, labelpad=10)\n",
    "\n",
    "# Plot 2: Map of billion dollar clusters\n",
    "vmin = billion_dollar['claim_sum'].min()\n",
    "vmax = billion_dollar['claim_sum'].max()\n",
    "norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "billion_dollar['color'] = billion_dollar['claim_sum'].apply(lambda x: cmap_yellow_red(norm(x)) if not np.isnan(x) else 'gray')\n",
    "\n",
    "gdf_counties.plot(ax=ax_map2, color='lightgray', edgecolor='gray', alpha=0.5)\n",
    "gdf_states.boundary.plot(ax=ax_map2, color='black', linewidth=0.5)\n",
    "\n",
    "patches2 = []\n",
    "for _, row in billion_dollar.iterrows():\n",
    "    center = (row['jittered_longitude'], row['jittered_latitude'])\n",
    "    radius = row['radius']\n",
    "    color = row['color']\n",
    "    circle = Circle(center, radius, edgecolor=color, facecolor=color, alpha=0.6)\n",
    "    patches2.append(circle)\n",
    "\n",
    "    ax_map2.text(\n",
    "        center[0], center[1], str(row['year']),\n",
    "        color='black', fontsize=9, style='italic', fontweight='bold', ha='center', va='center', zorder=5\n",
    "    )\n",
    "\n",
    "p2 = PatchCollection(patches2, match_original=True, transform=ax_map2.transData)\n",
    "ax_map2.add_collection(p2)\n",
    "\n",
    "ax_map2.set_xlim(extent[0], extent[1])\n",
    "ax_map2.set_ylim(extent[2], extent[3])\n",
    "ax_map2.axis('off')\n",
    "ax_map2.set_title(\"Billion-Dollar Hyperclustered Events\", fontsize=16)\n",
    "\n",
    "# Add subplot label \"b)\" in the upper left\n",
    "ax_map2.text(-0.1, 1, \"b)\", transform=ax_map2.transAxes, fontsize=14, va='top', ha='left')\n",
    "\n",
    "# Add second colorbar (left-aligned for the second map)\n",
    "cbar_ax2 = fig.add_axes([0.05, 0.1, 0.02, 0.3])  # Adjust position to align with the second map\n",
    "sm2 = plt.cm.ScalarMappable(cmap=cmap_yellow_red, norm=norm)\n",
    "cb2 = fig.colorbar(sm2, cax=cbar_ax2, orientation='vertical')\n",
    "cb2.ax.tick_params(labelsize=8)\n",
    "cb2.set_label('Cost ($)', fontsize=10, labelpad=10)\n",
    "\n",
    "# Bar Plot 1: Historical Disasters\n",
    "sns.barplot(ax=bar_axes[0], data=df2, x=\"Type\", y=\"Value\", palette=\"Greens\")\n",
    "bar_axes[0].set_title(\"Historical Disasters\", fontsize=10)\n",
    "bar_axes[0].set_ylabel(\"Claim Sum ($B)\")\n",
    "bar_axes[0].set_xlabel(\"\")\n",
    "bar_axes[0].set_xticklabels(bar_axes[0].get_xticklabels(), fontsize=8)\n",
    "\n",
    "# Add subplot label \"c)\" in the upper left\n",
    "bar_axes[0].text(-0.3, 1, \"c)\", transform=bar_axes[0].transAxes, fontsize=14, va='top', ha='left')\n",
    "\n",
    "sns.barplot(ax=bar_axes[1], data=top_8_clusters, x=\"Event\", y=\"claim_sum\", palette=\"Oranges\")\n",
    "bar_axes[1].set_title(\"99.9% Hyperclusters\", fontsize=10)\n",
    "bar_axes[1].set_ylabel(\"Claim Sum ($B)\")\n",
    "bar_axes[1].set_xlabel(\"\")\n",
    "bar_axes[1].set_xticklabels(bar_axes[1].get_xticklabels(), fontsize=8, rotation=45)\n",
    "\n",
    "# Add horizontal lines to the third bar plot\n",
    "bar_axes[1].axhline(sum_2024/1000000000, color='red', linestyle='--', label=\"2024 Premiums\")\n",
    "bar_axes[1].axhline(sum_hist/1000000000, color='blue', linestyle='--', label=\"Historic Premiums\")\n",
    "bar_axes[1].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Add subplot label \"d)\" in the upper left\n",
    "bar_axes[1].text(-0.3, 1, \"d)\", transform=bar_axes[1].transAxes, fontsize=14, va='top', ha='left')\n",
    "\n",
    "if save:\n",
    "    output_path = 'Plots/F2_Clusters.png'\n",
    "    plt.savefig(output_path, dpi=500, bbox_inches='tight')\n",
    "\n",
    "# Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e5348-8667-44be-b45c-5decd8b25317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
